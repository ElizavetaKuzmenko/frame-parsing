{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:08:34.720225",
     "start_time": "2017-03-01T01:08:32.802767"
    },
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../libs/')\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(31)\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:15:19.325557",
     "start_time": "2017-03-01T01:14:21.037196"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "input_data_path = '../data/annotated_corpus_fixed+syntaxnet.json'\n",
    "\n",
    "with open(input_data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print 'Number of examples: ', len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:34.268816",
     "start_time": "2017-03-01T01:09:31.550943"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from framebank_preprocessing.convert_corpus_to_brat import make_text, create_verb_example_index\n",
    "\n",
    "verb_index = create_verb_example_index(data)\n",
    "\n",
    "print 'Original number of verbs: ', len(verb_index)\n",
    "\n",
    "stat = sorted([(verb, len(examples)) for verb, examples in verb_index.iteritems()], \n",
    "              key = lambda x: x[1], reverse=True)\n",
    "\n",
    "verbs_to_keep = [verb for verb, count in stat if count >= 10]\n",
    "print 'Number of selected verbs: ', len(verbs_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:34.337459",
     "start_time": "2017-03-01T01:09:34.270680"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "examples = list()\n",
    "\n",
    "for verb in verbs_to_keep:\n",
    "    indexes = verb_index[verb]\n",
    "    \n",
    "    for ind in indexes:\n",
    "        examples.append((ind, data[ind]))\n",
    "\n",
    "print 'Number of framebank text examples for selected verbs:', len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:34.362099",
     "start_time": "2017-03-01T01:09:34.339318"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text, _ = make_text(examples[0][1], 0)\n",
    "\n",
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:34.387234",
     "start_time": "2017-03-01T01:09:34.363781"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "examples[0][1][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:34.810813",
     "start_time": "2017-03-01T01:09:34.389183"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MAX_VALUE = 50000\n",
    "\n",
    "def get_words_around(address, example, n):\n",
    "    it = n\n",
    "    curr = 0\n",
    "    result = list()\n",
    "    while it > 0:\n",
    "        index = address[1] - curr\n",
    "        if index < 0:\n",
    "            result.append(-1)\n",
    "            it -= 1\n",
    "        elif 'feat' in example[address[0]][index]:\n",
    "            result.append(index)\n",
    "            it -= 1\n",
    "        \n",
    "        curr += 1\n",
    "    \n",
    "    curr = 1\n",
    "    it = n\n",
    "    while it > 0:\n",
    "        index = address[1] + curr\n",
    "        if index >= len(example[address[0]]):\n",
    "            result.append(MAX_VALUE)\n",
    "            it -= 1\n",
    "        elif 'feat' in example[address[0]][index]:\n",
    "            result.append(index)\n",
    "            it -= 1\n",
    "        \n",
    "        curr += 1\n",
    "    \n",
    "    return sorted(result)\n",
    "\n",
    "\n",
    "def get_pos(feat_vec):\n",
    "    pos = feat_vec[0].strip() if feat_vec else str()\n",
    "    \n",
    "    replace_dict = {u'S-PRO' : u'SPRO',\n",
    "                   u'A-PRO' : u'APRO',\n",
    "                   u'ADV-PRO' : u'ADVPRO',\n",
    "                   u'PRAEDIC-PRO' : 'PRAEDICPRO'}\n",
    "    \n",
    "    if pos in replace_dict:\n",
    "        return replace_dict[pos]\n",
    "    else:\n",
    "        return pos\n",
    "\n",
    "    \n",
    "RUSVECTORES_EMBEDDINGS = True\n",
    "    \n",
    "\n",
    "def get_seq_lemmas(seq, ex_sent, word_num, special_tag):\n",
    "    result = list()\n",
    "    \n",
    "    for e in seq:\n",
    "        if e < 0:\n",
    "            result.append(u'')\n",
    "        elif e == MAX_VALUE:\n",
    "            result.append(u'')\n",
    "        else:\n",
    "            lemma = str()\n",
    "            morph = str()\n",
    "            if 'lemma' in ex_sent[e]:\n",
    "                lemma = ex_sent[e]['lemma']\n",
    "            else:\n",
    "                lemma = ex_sent[e]['form']\n",
    "            \n",
    "            if 'feat' in ex_sent[e]:\n",
    "                morph = get_pos(ex_sent[e]['feat'])\n",
    "            \n",
    "            if e == word_num:\n",
    "                morph = special_tag\n",
    "            \n",
    "            if RUSVECTORES_EMBEDDINGS:\n",
    "                result.append(lemma + '_' + morph)\n",
    "            else:\n",
    "                result.append(lemma.encode('utf8'))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def split_feats(feat):\n",
    "    return re.split('=|,| |\\|', feat)\n",
    "\n",
    "\n",
    "ARG_SPECIAL_TAG = 'ARGSPECIAL'\n",
    "PRED_SPECIAL_TAG = 'PREDSPECIAL'\n",
    "\n",
    "\n",
    "def parse_morph_features(feat_str):\n",
    "    splitted_feats = split_feats(feat_str)\n",
    "    \n",
    "    def make_binary_feat(feat, feat_true, feat_false):\n",
    "        result = 0.\n",
    "        if feat_true in feat:\n",
    "            result = 1.\n",
    "        elif feat_false in feat:\n",
    "            result = -1.\n",
    "        return result\n",
    "            \n",
    "    def make_multicategorial_feat(feat, feats):\n",
    "        result = u''\n",
    "        for categ in feats:\n",
    "            if categ in feat:\n",
    "                return categ\n",
    "        \n",
    "        return u'ABSENT'\n",
    "    \n",
    "    def prepare_feat(feat, feats):\n",
    "        if len(feats) > 2:\n",
    "            return make_multicategorial_feat(feat, feats)\n",
    "        else:\n",
    "            return make_binary_feat(feat, feats[0], feats[1])\n",
    "    \n",
    "    anim = prepare_feat(splitted_feats, [u'anim', u'inan'])\n",
    "    vform = prepare_feat(splitted_feats, [u'pf', u'ipf'])\n",
    "    case = prepare_feat(splitted_feats, ['nom', 'gen', 'dat', 'dat2', 'acc', 'ins', \n",
    "                                         'loc', 'gen2', 'acc2', 'loc2', 'voc', 'adnum'])\n",
    "    #case = splitted_feats[-1] if splitted_feats else str()\n",
    "    zform = prepare_feat(splitted_feats, [u'act', u'pass', u'med'])\n",
    "    shform = prepare_feat(splitted_feats, [u'brev', u'plen'])\n",
    "    pform = prepare_feat(splitted_feats, [u'intr', u'tran'])\n",
    "    vvform = prepare_feat(splitted_feats, [u'inf', u'partcp', u'ger'])\n",
    "    nform = prepare_feat(splitted_feats, [u'indic', u'imper', u'imper2'])\n",
    "    time = prepare_feat(splitted_feats, [u'praet', u'praes', u'fut'])\n",
    "        \n",
    "    return {\n",
    "        'pos' : get_pos(splitted_feats),\n",
    "        'case' : case,\n",
    "        'anim' : anim,\n",
    "        'vform' : vform,\n",
    "        'zform' : zform,\n",
    "        'shform' : shform,\n",
    "        'pform' : pform,\n",
    "        'vvform' : vvform,\n",
    "        'nform' : nform,\n",
    "        'time' : time\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_preposition(arg, example):\n",
    "    sent = example[arg[0]]\n",
    "    arg_word = sent[arg[1]]\n",
    "    \n",
    "    for word in sent:\n",
    "        if 'parent' not in word:\n",
    "            continue\n",
    "        \n",
    "        if word['parent'] != arg[1]:\n",
    "            continue\n",
    "        \n",
    "        if word['postag_p'] != u'ADP':\n",
    "            continue\n",
    "        \n",
    "        if 'lemma' not in word:\n",
    "            continue\n",
    "        \n",
    "        return word['lemma']\n",
    "    \n",
    "    return u'ABSENT'\n",
    "\n",
    "\n",
    "def process_arg_pred(ex_id, pred, args, example):\n",
    "    feature_sets = list()\n",
    "    \n",
    "    for arg in args:\n",
    "        pred_word = example[pred[0]][pred[1]]\n",
    "        arg_word = example[arg[0]][arg[1]]\n",
    "        \n",
    "        word_indexes = get_words_around(arg, example, 1)\n",
    "        arg_context_lemmas = get_seq_lemmas(word_indexes, \n",
    "                                            example[arg[0]],\n",
    "                                            arg[1],\n",
    "                                            ARG_SPECIAL_TAG)\n",
    "        \n",
    "        word_indexes = get_words_around(pred, example, 5)\n",
    "        pred_context_lemmas = get_seq_lemmas(word_indexes, \n",
    "                                             example[pred[0]],\n",
    "                                             pred[1],\n",
    "                                             PRED_SPECIAL_TAG)\n",
    "        \n",
    "        arg_feat = arg_word['feat'] if 'feat' in arg_word else str()\n",
    "        splitted_arg_feat = split_feats(arg_feat)\n",
    "        arg_pos = get_pos(splitted_arg_feat)\n",
    "        arg_case = splitted_arg_feat[-1] if splitted_arg_feat else str()\n",
    "        \n",
    "        pred_feat = pred_word['feat'] if 'feat' in pred_word else str()\n",
    "        splitted_pred_feat = split_feats(pred_feat)\n",
    "        pred_pos = get_pos(splitted_pred_feat)\n",
    "        \n",
    "        if RUSVECTORES_EMBEDDINGS:\n",
    "            arg_lemma = arg_word['lemma'] + u'_' + arg_pos if 'lemma' in arg_word else str()\n",
    "            pred_lemma = pred_word['lemma'] + u'_' + pred_pos if 'lemma' in pred_word else str()\n",
    "        else:\n",
    "            arg_lemma = arg_word['lemma'].encode('utf8') if 'lemma' in arg_word else str()\n",
    "            pred_lemma = pred_word['lemma'].encode('utf8') if 'lemma' in pred_word else str()\n",
    "        \n",
    "        dist = 1. * abs(arg[1] - pred[1]) if pred[0] == arg[0] else 10.\n",
    "        \n",
    "        link_name = arg_word['link_name'] if 'link_name' in arg_word else str()\n",
    "        \n",
    "        arg_prep = extract_preposition(arg, example)\n",
    "        \n",
    "        tag_features = lambda string, ff: {string + u'_' + k : v for k, v in parse_morph_features(ff).iteritems()}\n",
    "        arg_feat_res = tag_features('arg', arg_feat)\n",
    "    \n",
    "        pred_feat_res = tag_features('pred', pred_feat)\n",
    "        \n",
    "        features = {'pred_lemma' : pred_lemma, \n",
    "                    'arg_lemma' : arg_lemma,\n",
    "                    'arg_context_lemmas' : arg_context_lemmas,\n",
    "                    'pred_context_lemmas' : pred_context_lemmas,\n",
    "                    'role' : arg_word['rolepred1'],\n",
    "                    'rel_pos' : 1. if arg < pred else -1.,\n",
    "                    'dist' : dist,\n",
    "                    'arg_prep' : arg_prep,\n",
    "                    'link_name' : link_name,\n",
    "                    'ex_id' : ex_id,\n",
    "                    'arg_address' : arg\n",
    "                   }\n",
    "        features.update(arg_feat_res)\n",
    "        features.update(pred_feat_res)\n",
    "        \n",
    "        feature_sets.append(features)\n",
    "    \n",
    "    return feature_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:34.833553",
     "start_time": "2017-03-01T01:09:34.813312"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_example(ex_id, sentences):\n",
    "    pred = None\n",
    "    args = list()\n",
    "    for sent_num, sent in enumerate(sentences):\n",
    "        for word_num, word in enumerate(sent):\n",
    "            if 'rank' in word and word['rank'] == u'Предикат':\n",
    "                pred = (sent_num, word_num)\n",
    "            elif 'rolepred1' in word:\n",
    "                args.append((sent_num, word_num))\n",
    "    \n",
    "    return process_arg_pred(ex_id, pred, args, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:45.876343",
     "start_time": "2017-03-01T01:09:34.835217"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_features_and_make_dataframe(examples):\n",
    "    feature_sets = list()\n",
    "    for ex_id, ex in examples:\n",
    "        feature_sets += process_example(ex_id, ex)\n",
    "\n",
    "    print 'Number of dataset objects:', len(feature_sets)\n",
    "    \n",
    "    pd_data = pd.DataFrame(feature_sets)\n",
    "    pd_data = pd_data.sample(frac=1) # Shuffeling data for neural network\n",
    "    \n",
    "    return pd_data\n",
    "\n",
    "pd_data = extract_features_and_make_dataframe(examples)\n",
    "pd_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:54.753226",
     "start_time": "2017-03-01T01:09:45.880186"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Label preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:55.121543",
     "start_time": "2017-03-01T01:09:55.024505"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_stat = pd_data.loc[:, 'role'].value_counts()\n",
    "drop_ys = y_stat[y_stat < 180].index\n",
    "clear_data = pd_data.drop(pd_data[pd_data.loc[:, 'role'].isin(drop_ys)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:55.666595",
     "start_time": "2017-03-01T01:09:55.123621"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "repl_roles = {\n",
    "    u'агенс - субъект восприятия' : u'субъект восприятия',\n",
    "    u'агенс - субъект ментального состояния' : u'субъект ментального состояния',\n",
    "    u'результат / цель' : u'результат',\n",
    "    u'место - пациенс' : u'место',\n",
    "    u'говорящий - субъект психологического состояния' : u'субъект психологического состояния'\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_single_region(data, rep, val):\n",
    "    data.loc[:, 'role'] = data.loc[:, 'role'].str.replace(rep, val)\n",
    "\n",
    "\n",
    "for rep, val in repl_roles.iteritems():\n",
    "    normalize_single_region(clear_data, rep, val)\n",
    "    \n",
    "number_of_roles = len(clear_data.loc[:, 'role'].value_counts().index)\n",
    "print 'Number of roles: ', number_of_roles\n",
    "clear_data.loc[:, 'role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:55.724308",
     "start_time": "2017-03-01T01:09:55.668444"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_orig = clear_data.loc[:, 'role']\n",
    "X_orig = clear_data.drop('role', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:56.806290",
     "start_time": "2017-03-01T01:09:55.727542"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "y = label_encoder.fit_transform(y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:09:56.817934",
     "start_time": "2017-03-01T01:09:56.810247"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:10:04.082134",
     "start_time": "2017-03-01T01:09:56.821698"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "embeddings_path = '../data/ruscorpora_mean_hs.model.bin'\n",
    "embeddings = Word2Vec.load_word2vec_format(embeddings_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:10:04.144550",
     "start_time": "2017-03-01T01:10:04.086771"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_embeddings_length(embeddings):\n",
    "    if RUSVECTORES_EMBEDDINGS:\n",
    "        return embeddings[u'стоять_V'].shape[0]\n",
    "    else:\n",
    "        return embeddings[u'бежать'.encode('utf8')].shape[0]\n",
    "\n",
    "\n",
    "def make_embeded_form(word):\n",
    "    if word:\n",
    "        return u\"{}_{}\".format(word[1], word[0])\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "class Embedder_map(object):\n",
    "    def __init__(self, X):\n",
    "        self.X_ = X\n",
    "        self.embeddings_length_ = get_embeddings_length(embeddings)\n",
    "\n",
    "    def __call__(self, i):  \n",
    "        result = np.zeros((len(self.X_[0]), self.embeddings_length_))\n",
    "\n",
    "        for j in xrange(len(self.X_[0])):\n",
    "            word = self.X_[i][j]\n",
    "            tag = word[0] if word else str()\n",
    "            #word = make_embeded_form(word)\n",
    "            if tag == ARG_SPECIAL_TAG or tag == ARG_SPECIAL_TAG:\n",
    "                result[j, :] = np.ones(self.embeddings_length_)\n",
    "            elif word and word in embeddings:\n",
    "                result[j, :] = embeddings[word]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def embed(X):\n",
    "    pool = mp.Pool(4)\n",
    "    result = pool.map(Embedder_map(X), X.index, 1000)\n",
    "    pool.close()\n",
    "        \n",
    "    return np.asarray(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:10:37.061643",
     "start_time": "2017-03-01T01:10:04.147333"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "arg_context_embedded = embed(X_orig.loc[:, 'arg_context_lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:24.210551",
     "start_time": "2017-03-01T01:10:37.063753"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred_context_embedded = embed(X_orig.loc[:, 'pred_context_lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:24.225518",
     "start_time": "2017-03-01T01:11:24.212540"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Embedder_single_map(object):\n",
    "    def __init__(self, X):\n",
    "        self.X_ = X\n",
    "        self.embeddings_length_ = get_embeddings_length(embeddings)\n",
    "\n",
    "    def __call__(self, i):\n",
    "        #word = make_embeded_form(self.X_[i])\n",
    "        word = self.X_[i]\n",
    "        if word in embeddings:\n",
    "            return embeddings[word]\n",
    "        else:\n",
    "            return np.zeros((self.embeddings_length_,))\n",
    "\n",
    "        \n",
    "def embed_single(X):\n",
    "    pool = mp.Pool(4)\n",
    "    result = pool.map(Embedder_single_map(X), X.index, 1000)\n",
    "    \n",
    "    pool.close()\n",
    "        \n",
    "    return np.asarray(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:31.628802",
     "start_time": "2017-03-01T01:11:24.227055"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embedded_verbs = embed_single(X_orig.pred_lemma)\n",
    "print embedded_verbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:31.737501",
     "start_time": "2017-03-01T01:11:31.630949"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(np.linalg.norm(embedded_verbs, axis = 1) < 0.001).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:31.900059",
     "start_time": "2017-03-01T01:11:31.739521"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clear_data[(np.linalg.norm(embedded_verbs, axis = 1) < 0.001)].pred_lemma.value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:38.595787",
     "start_time": "2017-03-01T01:11:31.902007"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embedded_args = embed_single(X_orig.arg_lemma)\n",
    "print embedded_args.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:38.720508",
     "start_time": "2017-03-01T01:11:38.599563"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(np.linalg.norm(embedded_args, axis = 1) < 0.001).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Vectorizing categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:38.742286",
     "start_time": "2017-03-01T01:11:38.722747"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "def vectorize_categorial_features(feat_list):   \n",
    "    categ_feats = [e for e in feat_list if X_orig.loc[:,e].dtype in [str, unicode, object]]\n",
    "    not_categ = [e for e in feat_list if e not in categ_feats]\n",
    "    print 'Category features:\\n', categ_feats\n",
    "    print 'Not category features:\\n', not_categ\n",
    "    \n",
    "    vectorizer = DictVectorizer(sparse = False)\n",
    "    one_hot_feats = vectorizer.fit_transform(X_orig.loc[:, categ_feats].to_dict(orient = 'records'))\n",
    "    print one_hot_feats.shape\n",
    "    \n",
    "    not_categ_columns = np.concatenate(tuple(X_orig.loc[:, e].as_matrix().reshape(-1, 1) \n",
    "                                             for e in not_categ), axis =1)\n",
    "    no_lemma_plain_features = np.concatenate((not_categ_columns,\n",
    "                                              one_hot_feats), axis = 1)\n",
    "    \n",
    "    return no_lemma_plain_features, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:42.985695",
     "start_time": "2017-03-01T01:11:38.744139"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "morph_feats = ['pos', 'case', 'anim', 'vform', 'zform', 'shform', 'pform', 'vvform', 'nform', 'time']\n",
    "morph_feats_arg_pred = ['arg_' + e for e in morph_feats] + ['pred_' + e for e in morph_feats]\n",
    "all_feats_no_pred_lemma = morph_feats_arg_pred + ['rel_pos', 'arg_prep', 'link_name'] \n",
    "no_lemma_plain_features, categ_feat_vecorizer = vectorize_categorial_features(all_feats_no_pred_lemma)\n",
    "print 'Categorical features without predicate lemma shape:', no_lemma_plain_features.shape\n",
    "\n",
    "pred_lemma_vectorizer = DictVectorizer(sparse = False)\n",
    "pred_lemma_feats = pred_lemma_vectorizer.fit_transform(X_orig.loc[:, ['pred_lemma']].to_dict(orient = 'records'))\n",
    "print 'Pred lemma features shape:', pred_lemma_feats.shape\n",
    "\n",
    "plain_features = np.concatenate((no_lemma_plain_features,\n",
    "                                 pred_lemma_feats), axis = 1)\n",
    "print 'All categorical features shape:', plain_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Out of domain split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:43.012789",
     "start_time": "2017-03-01T01:11:42.988243"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def make_dist_matrix(pd_data, embeddings):\n",
    "    verb_counts = pd_data.pred_lemma.value_counts()\n",
    "    dist_matrix = np.zeros((len(verb_counts), len(verb_counts)))\n",
    "\n",
    "    for i in xrange(dist_matrix.shape[0]):\n",
    "        left = verb_counts.index[i]\n",
    "        if left not in embeddings:\n",
    "            continue\n",
    "\n",
    "        left_embed = embeddings[left]\n",
    "        for j in xrange(i, dist_matrix.shape[0]):\n",
    "            right = verb_counts.index[j]\n",
    "            if right not in embeddings:\n",
    "                continue\n",
    "\n",
    "            right_embed = embeddings[right]\n",
    "            dist = 1 - cosine(left_embed.reshape(-1,1), right_embed.reshape(-1,1))\n",
    "            dist_matrix[i, j] = dist\n",
    "    \n",
    "    return dist_matrix, verb_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:43.037658",
     "start_time": "2017-03-01T01:11:43.014498"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sort_verb_pairs(dist_matrix, verb_counts):\n",
    "    verb_pairs = list()\n",
    "    for i in xrange(dist_matrix.shape[0]):\n",
    "        for j in xrange(i + 1, dist_matrix.shape[0]):\n",
    "            verb_pairs.append((i, j, dist_matrix[i,j]))\n",
    "            \n",
    "    sresult = sorted(verb_pairs, key= lambda k: k[2], reverse=True) # good split\n",
    "    #sresult = sorted(verb_pairs, key= lambda k: k[2]) # bad split\n",
    "    \n",
    "    vresult = [(verb_counts.index[e[0]], verb_counts.index[e[1]], e[2]) for e in sresult[:30]]\n",
    "    for v in vresult:\n",
    "        print u'({}, {}, {})'.format(v[0], v[1], v[2])\n",
    "    \n",
    "    return vresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:43.069513",
     "start_time": "2017-03-01T01:11:43.039112"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_out_of_domain_verbs(vresult):\n",
    "    out_of_domain_verbs = set()\n",
    "    in_domain_verbs = set()\n",
    "\n",
    "    for v in vresult:\n",
    "        if v[0] not in out_of_domain_verbs:\n",
    "            if v[0] not in in_domain_verbs:\n",
    "                out_of_domain_verbs.add(v[0])\n",
    "                in_domain_verbs.add(v[1])\n",
    "        else:\n",
    "            if v[1] not in in_domain_verbs:\n",
    "                out_of_domain_verbs.add(v[1])\n",
    "                in_domain_verbs.add(v[0])\n",
    "    \n",
    "    return list(out_of_domain_verbs), in_domain_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:47.470129",
     "start_time": "2017-03-01T01:11:43.070940"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dist_matrix, verb_counts = make_dist_matrix(pd_data, embeddings)\n",
    "srt_verbs = sort_verb_pairs(dist_matrix, verb_counts)\n",
    "ood_verbs, ind_verbs = get_out_of_domain_verbs(srt_verbs)\n",
    "\n",
    "print\n",
    "print 'Out-of domain-verbs'\n",
    "print len(ood_verbs)\n",
    "for e in ood_verbs:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.378777",
     "start_time": "2017-03-01T01:11:47.471748"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "selector = X_orig.pred_lemma.isin(ood_verbs).as_matrix()\n",
    "\n",
    "ood_plain_features = no_lemma_plain_features[selector]\n",
    "ind_plain_features = no_lemma_plain_features[~selector]\n",
    "\n",
    "ood_y = y[selector]\n",
    "ind_y = y[~selector]\n",
    "\n",
    "ood_arg_context = arg_context_embedded[selector]\n",
    "ind_arg_context = arg_context_embedded[~selector]\n",
    "\n",
    "ood_pred_context = pred_context_embedded[selector]\n",
    "ind_pred_context = pred_context_embedded[~selector]\n",
    "\n",
    "ood_arg_embed = embedded_args[selector]\n",
    "ind_arg_embed = embedded_args[~selector]\n",
    "\n",
    "ood_pred_embed = embedded_verbs[selector]\n",
    "ind_pred_embed = embedded_verbs[~selector]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.385438",
     "start_time": "2017-03-01T01:11:48.380809"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print ind_pred_embed.shape\n",
    "ood_pred_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.413660",
     "start_time": "2017-03-01T01:11:48.386949"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print 'Baseline for ood evaluation:'\n",
    "gold_pred = ood_y.argmax(axis = 1)\n",
    "baseline_pred = pd.Series(gold_pred).value_counts().index[0] * np.ones(gold_pred.shape)\n",
    "\n",
    "f1_micro = f1_score(baseline_pred, gold_pred, average = 'micro')\n",
    "f1_macro = f1_score(baseline_pred, gold_pred, average = 'macro')\n",
    "print 'micro', f1_micro\n",
    "print 'macro', f1_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.430174",
     "start_time": "2017-03-01T01:11:48.415148"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Permute\n",
    "from keras.layers import merge\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Merge\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2, activity_l2, l1\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import MaxPooling1D\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.461618",
     "start_time": "2017-03-01T01:11:48.431833"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def construct_plain_model(input_shape):\n",
    "    print 'Plain model.'\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(600, \n",
    "                          #input_shape=(plain_features.shape[1],), \n",
    "                          input_shape = input_shape,\n",
    "                          activation = 'relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(400))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(number_of_roles))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('softmax'))\n",
    "    \n",
    "    plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return plain_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.493868",
     "start_time": "2017-03-01T01:11:48.463224"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def construct_plain_model_sparse(input_shape, n_embedding_vecs = 2):\n",
    "    print 'Complex model.'\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(400, input_shape = input_shape))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('relu'))\n",
    "    \n",
    "    def create_embed_model():\n",
    "        embed_model = Sequential()\n",
    "        embed_model.add(Dense(100, input_shape = (get_embeddings_length(embeddings), )))\n",
    "        embed_model.add(BatchNormalization())\n",
    "        embed_model.add(Activation('relu'))\n",
    "        \n",
    "        return embed_model\n",
    "    \n",
    "    embed_models = [create_embed_model() for i in xrange(n_embedding_vecs)]\n",
    "#     arg_embed_model = create_embed_model()\n",
    "#     pred_embed_model = create_embed_model()\n",
    "    \n",
    "    final = Sequential()\n",
    "#     final.add(Merge([arg_embed_model, pred_embed_model, plain_model], \n",
    "#                     mode = 'concat', concat_axis=1))\n",
    "    final.add(Merge(embed_models + [plain_model], \n",
    "                    mode = 'concat', concat_axis = 1))\n",
    "    final.add(Dropout(0.3))\n",
    "    final.add(Dense(400))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('relu'))\n",
    "    final.add(Dropout(0.3))\n",
    "    \n",
    "    final.add(Dense(number_of_roles))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('softmax'))\n",
    "    \n",
    "    final.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return final\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Context model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.548963",
     "start_time": "2017-03-01T01:11:48.495479"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Masking\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "def construct_graph_lstm_model(plain_features_shape):\n",
    "    print 'Context model.'\n",
    "    \n",
    "    def create_embed_model():\n",
    "        embed_model = Sequential()\n",
    "        embed_model.add(Dense(100, input_shape = (get_embeddings_length(embeddings), )))\n",
    "        embed_model.add(BatchNormalization())\n",
    "        embed_model.add(Activation('relu'))\n",
    "        return embed_model\n",
    "    \n",
    "    def construct_attentional_part(context_length):\n",
    "        seq_model = Sequential()\n",
    "        seq_model.add(Convolution1D(nb_filter=50, \n",
    "                                    filter_length=1, \n",
    "                                    border_mode='same', \n",
    "                                    activation='relu',\n",
    "                                    input_shape = (context_length, \n",
    "                                                   get_embeddings_length(embeddings))))\n",
    "#         seq_model.add(Masking(mask_value=0., input_shape = (context_length, \n",
    "#                                                             get_embeddings_length(embeddings))))\n",
    "        #seq_model.add(Masking(mask_value=1.))\n",
    "        seq_model.add(Bidirectional(LSTM(50), merge_mode='sum'))\n",
    "        #seq_model.add(LSTM(100))\n",
    "        seq_model.add(Dense(50))\n",
    "        seq_model.add(BatchNormalization())\n",
    "        seq_model.add(Activation('relu'))\n",
    "        \n",
    "        return seq_model\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    #arg_context_model = construct_attentional_part(arg_context_embedded.shape[1])\n",
    "    pred_context_model = construct_attentional_part(pred_context_embedded.shape[1])\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(400, input_shape = plain_features_shape))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('relu'))\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    arg_embed_model = create_embed_model()\n",
    "    pred_embed_model = create_embed_model()\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    final1 = Sequential()\n",
    "    final1.add(Merge([\n",
    "  #              arg_context_model, \n",
    "                     #pred_context_model,\n",
    "                     arg_embed_model,\n",
    "                     pred_embed_model,\n",
    "                     plain_model], \n",
    "                    mode = 'concat', concat_axis=1))\n",
    "    final1.add(Dropout(0.3))\n",
    "    \n",
    "    final1.add(Dense(400))\n",
    "    final1.add(BatchNormalization())\n",
    "    final1.add(Activation('relu'))\n",
    "    final1.add(Dropout(0.3))\n",
    "    \n",
    "    final = Sequential()\n",
    "    final.add(Merge([final1, pred_context_model], mode = 'concat', concat_axis = 1))\n",
    "    \n",
    "    final.add(Dense(number_of_roles))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('softmax'))\n",
    "    \n",
    "    final.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.600206",
     "start_time": "2017-03-01T01:11:48.550589"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def construct_graph_attentional_model():\n",
    "    print 'Context attentional model.'\n",
    "    \n",
    "    def construct_attentional_part(context_length):\n",
    "        _input = Input(shape = (context_length, \n",
    "                                get_embeddings_length(embeddings)), dtype = 'float')\n",
    "\n",
    "        conv = Convolution1D(nb_filter=200, \n",
    "                            filter_length=2, \n",
    "                            border_mode='same', \n",
    "                            activation='relu')(_input)\n",
    "\n",
    "        units = 100\n",
    "        activations = LSTM(units, return_sequences=True)(conv)\n",
    "\n",
    "        # compute importance for each step\n",
    "        attention = TimeDistributed(Dense(1, activation='tanh'))(activations)  \n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(units)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        # apply the attention\n",
    "        seq_repr = merge([activations, attention], mode='mul')\n",
    "        seq_repr = Lambda(lambda xin: K.sum(xin, axis=1))(seq_repr)\n",
    "        seq_model = Model(input=_input, output=seq_repr)\n",
    "        \n",
    "        return seq_model\n",
    "    \n",
    "    arg_context_model = construct_attentional_part(arg_context_embedded.shape[1])\n",
    "    pred_context_model = construct_attentional_part(pred_context_embedded.shape[1])\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(800, \n",
    "                          input_shape=(plain_features.shape[1],), \n",
    "                          activation = 'relu'))\n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    final = Sequential()\n",
    "    final.add(Merge([arg_context_model, pred_context_model, plain_model], \n",
    "                    mode = 'concat', concat_axis=1))\n",
    "    final.add(Dropout(0.3))\n",
    "    \n",
    "    #final.add(Dense(300, activation = 'relu'))\n",
    "    final.add(Dense(400))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('relu'))\n",
    "    final.add(Dropout(0.3))\n",
    "    \n",
    "    final.add(Dense(number_of_roles))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('softmax'))\n",
    "    #final.add(Dense(number_of_roles, activation = 'softmax'))\n",
    "#    final.add(BatchNormalization())\n",
    "    #final.add(Activation('softmax'), W_regularizer=l2(0.01))\n",
    "    #final.add(Dense(number_of_roles, activation='softmax', W_regularizer = l2(0.01)))\n",
    "    \n",
    "    final.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.625724",
     "start_time": "2017-03-01T01:11:48.601723"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "def construct_graph_bidirectional_model():\n",
    "    print 'Bidirectional model.'\n",
    "    \n",
    "    arg_context_model = Sequential()\n",
    "    arg_context_model.add(Convolution1D(nb_filter=150, \n",
    "                                        filter_length=2, \n",
    "                                        border_mode='same', \n",
    "                                        activation='relu',\n",
    "                                        input_shape = (arg_context_embedded.shape[1], \n",
    "                                                       get_embeddings_length(embeddings))))\n",
    "    arg_context_model.add(Bidirectional(LSTM(100), merge_mode = 'sum'))\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(700, \n",
    "                          input_shape=(plain_features.shape[1],), \n",
    "                          activation = 'relu'))\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    final = Sequential()\n",
    "    final.add(Merge([arg_context_model, plain_model], mode = 'concat', concat_axis=1))\n",
    "    final.add(Dropout(0.3))\n",
    "    \n",
    "    #final.add(Dense(300, activation = 'relu'))\n",
    "    final.add(Dense(300))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('relu'))\n",
    "    final.add(Dropout(0.3))\n",
    "    \n",
    "    final.add(Dense(number_of_roles))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('softmax'))\n",
    "    \n",
    "    final.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.657833",
     "start_time": "2017-03-01T01:11:48.627169"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def construct_simple_attentional_model():\n",
    "    units = 80\n",
    "    _input = Input(shape = (arg_context_embedded.shape[1], \n",
    "                            get_embeddings_length(embeddings)), dtype = 'float')\n",
    "\n",
    "    conv = Convolution1D(nb_filter=128, \n",
    "                        filter_length=2, \n",
    "                        border_mode='same', \n",
    "                        activation='relu')(_input)\n",
    "\n",
    "    activations = LSTM(units, return_sequences=True)(conv)\n",
    "\n",
    "    # compute importance for each step\n",
    "    attention = TimeDistributed(Dense(1, activation='tanh'))(activations) \n",
    "    #attention = Dense(6, activation='tanh')(activations) \n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(units)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "\n",
    "    # apply the attention\n",
    "    sent_representation = merge([activations, attention], mode='mul')\n",
    "    sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
    "\n",
    "    #dn = Dense(100, activation = 'tanh')(sent_representation)\n",
    "    #probabilities = Dense(number_of_roles, activation='softmax')(dn)\n",
    "    probabilities = Dense(number_of_roles, activation='softmax')(sent_representation)\n",
    "\n",
    "    model = Model(input=_input, output=probabilities)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.689451",
     "start_time": "2017-03-01T01:11:48.659373"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def construct_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(nb_filter=128, \n",
    "                            filter_length=2, \n",
    "                            border_mode='same', \n",
    "                            activation='relu', \n",
    "                            input_shape = (seq_embeded.shape[1], \n",
    "                                           get_embeddings_length(embeddings))))\n",
    "\n",
    "    #model.add(MaxPooling1D(pool_length=2))\n",
    "    model.add(LSTM(80))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(60, activation='tanh'))\n",
    "    model.add(Dense(number_of_roles, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Tuning for in-domain test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T17:19:35.072435",
     "start_time": "2017-02-20T17:19:32.312021"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = construct_plain_model((plain_features.shape[1],))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit(plain_features, y, nb_epoch=15, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-02-19T10:46:04.738Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model_sparse((plain_features.shape[1],))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([embedded_args, embedded_verbs, plain_features], y, \n",
    "          nb_epoch=15, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Context model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-02-18T22:17:10.003Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = construct_graph_lstm_model((plain_features.shape[1],))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([arg_context_embedded, pred_context_embedded, embedded_args, embedded_verbs, plain_features], y, \n",
    "          nb_epoch=15, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Tuning for out-of-domain test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = construct_plain_model((ind_plain_features.shape[1],))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit(ind_plain_features, ind_y, nb_epoch=15, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "\n",
    "#model.evaluate(ood_plain_features, ood_y)\n",
    "ev_res = evaluate_model(model, [ood_plain_features], ood_y)\n",
    "print \n",
    "print pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = construct_plain_model_sparse((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([ind_arg_embed, ind_pred_embed, ind_plain_features], ind_y, nb_epoch=20, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "#model.evaluate([ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "\n",
    "ev_res = evaluate_model(model, [ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "print \n",
    "print pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Context model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = construct_graph_lstm_model((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([\n",
    "           #ind_arg_context, \n",
    "        #ind_pred_context,   \n",
    "        ind_arg_embed, \n",
    "        ind_pred_embed, \n",
    "        ind_plain_features,\n",
    "        ind_pred_context], \n",
    "#model.fit([ind_arg_context, ind_pred_context, ind_arg_embed, ind_pred_embed, ind_plain_features], \n",
    "           ind_y, nb_epoch=6, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "\n",
    "model.evaluate([\n",
    "        ood_arg_embed, \n",
    "        ood_pred_embed,\n",
    "        ood_plain_features,\n",
    "        ood_pred_context\n",
    "    ], ood_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Evaluate for in domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.721630",
     "start_time": "2017-03-01T01:11:48.690849"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    keras_eval = model.evaluate(X_test, y_test)\n",
    "    pred = model.predict(X_test).argmax(axis = 1)\n",
    "    f1_micro = f1_score(pred, y_test.argmax(axis = 1), average = 'micro')\n",
    "    f1_macro = f1_score(pred, y_test.argmax(axis = 1), average = 'macro')\n",
    "    accur = accuracy_score(pred, y_test.argmax(axis = 1))\n",
    "    \n",
    "    return np.array(list(keras_eval) + [f1_micro, f1_macro, accur])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.799286",
     "start_time": "2017-03-01T01:11:48.723161"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Baseline(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X_train, y_train, *args, **kwargs):\n",
    "        self.pred_class_ = pd.Series(y_train.argmax(axis = 1)).value_counts().index[0]\n",
    "        self.class_num_ = y_train.shape[1]\n",
    "    \n",
    "    def predict(self, X_test, *args, **kwargs):\n",
    "        result = np.zeros((X_test[0].shape[0], self.class_num_))\n",
    "        result[:, self.pred_class_] = np.ones((X_test[0].shape[0],))\n",
    "        return result\n",
    "    \n",
    "    def evaluate(self, X_test, y_test, *args, **kwargs):\n",
    "        return (0., 0.)\n",
    "    \n",
    "    def summary(self):\n",
    "        print 'Baseline'\n",
    "        pass\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, *args, **kwargs):\n",
    "    model.fit(X_train, y_train, *args, **kwargs)\n",
    "    \n",
    "    keras_eval = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    pred = model.predict(X_test).argmax(axis = 1)\n",
    "    f1_micro = f1_score(pred, y_test.argmax(axis = 1), average = 'micro')\n",
    "    f1_macro = f1_score(pred, y_test.argmax(axis = 1), average = 'macro')\n",
    "    accur = accuracy_score(pred, y_test.argmax(axis = 1))\n",
    "    \n",
    "    return list(keras_eval) + [f1_micro, f1_macro, accur]\n",
    "    \n",
    "\n",
    "def custom_cross_val(cr_f, X, y, cv, *args, **kwargs):\n",
    "    cr_f().summary()\n",
    "    eval_res = list()\n",
    "    for i, (train, test) in enumerate(cv.split(y)):\n",
    "        model = cr_f()\n",
    "        print \"Running Fold\", i+1, \"/\", cv.n_splits\n",
    "        eval1 = train_and_evaluate_model(model, \n",
    "                                         [X[j][train] for j in xrange(len(X))], y[train], \n",
    "                                         [X[j][test] for j in xrange(len(X))], y[test], \n",
    "                                         *args, **kwargs)\n",
    "        \n",
    "        print\n",
    "        print 'Fold result: ', eval1\n",
    "        eval_res.append(eval1)\n",
    "    \n",
    "    return np.array(eval_res)\n",
    "\n",
    "\n",
    "def describe_cv_result(cv_res):\n",
    "    print cv_res\n",
    "    mean_cv_res = cv_res.mean(axis = 0)\n",
    "    std_cv_res = cv_res.std(axis = 0)\n",
    "    print 'Mean'\n",
    "    print pd.DataFrame([mean_cv_res], columns = ['loss', 'keras_accur', 'micro_f1', 'macro_f1', 'accur'])\n",
    "    print 'Std'\n",
    "    print pd.DataFrame([std_cv_res], columns = ['loss', 'keras_accur', 'micro_f1', 'macro_f1', 'accur'])\n",
    "\n",
    "    \n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T16:35:46.437017",
     "start_time": "2017-02-20T16:35:45.166455"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "curr_features = np.concatenate((no_lemma_plain_features, embedded_verbs), axis = 1)\n",
    "cv_res = custom_cross_val(lambda : Baseline(), \n",
    "                          [curr_features], \n",
    "                          y, cv = cv, nb_epoch=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### No predicate lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-19T11:05:37.420828",
     "start_time": "2017-02-19T10:59:02.359005"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "curr_features = np.concatenate((no_lemma_plain_features, embedded_verbs), axis = 1)\n",
    "cv_res = custom_cross_val(lambda : construct_plain_model((curr_features.shape[1],)), \n",
    "                          [curr_features], \n",
    "                          y, cv = cv, nb_epoch=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### All categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T15:36:12.375033",
     "start_time": "2017-02-16T15:27:40.793790"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_plain_model((plain_features.shape[1],)), \n",
    "                          [plain_features], \n",
    "                          y, cv = cv, nb_epoch=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Categorical features + embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T17:21:40.841561",
     "start_time": "2017-02-20T17:21:40.833325"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plain_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-19T12:50:12.742255",
     "start_time": "2017-02-19T12:42:54.008335"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "single_chunk = np.concatenate((embedded_args, embedded_verbs, plain_features), axis = 1)\n",
    "cv_res = custom_cross_val(lambda : construct_plain_model((single_chunk.shape[1],)), \n",
    "                          [single_chunk], \n",
    "                          y, cv = cv, nb_epoch=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-19T14:46:39.104869",
     "start_time": "2017-02-19T14:35:13.641904"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_plain_model_sparse((plain_features.shape[1],)), \n",
    "                          [embedded_args, embedded_verbs, plain_features], y, \n",
    "                          cv = cv, nb_epoch=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Context model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T14:16:02.109004",
     "start_time": "2017-02-16T13:49:33.683794"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_graph_lstm_model((plain_features.shape[1],)), \n",
    "                          [arg_context_embedded, \n",
    "                           pred_context_embedded, \n",
    "                           embedded_args, \n",
    "                           embedded_verbs,\n",
    "                           plain_features], y, \n",
    "                          cv = cv, nb_epoch=6, batch_size=64, validation_split = 0., \n",
    "                          shuffle=True)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Evaluate for out of domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:11:48.821577",
     "start_time": "2017-03-01T01:11:48.801010"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate_out_of_domain(model, X_train, y_train, X_test, y_test):\n",
    "    final_res = list()\n",
    "    N_ITERATIONS = 5\n",
    "    for i in xrange(N_ITERATIONS):\n",
    "        print 'Eval iter:', i + 1, '/', N_ITERATIONS\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                       patience=2, verbose=0, mode='auto')\n",
    "        model.fit(X_train, y_train, nb_epoch=15, \n",
    "                  batch_size=64, validation_split = 0.1, \n",
    "                  shuffle=True, callbacks = [early_stopping],\n",
    "                 verbose = 0)\n",
    "\n",
    "        ev_res = evaluate_model(model, X_test, y_test)\n",
    "        print \n",
    "        print pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur'])\n",
    "        final_res.append(ev_res)\n",
    "    \n",
    "    return np.array(final_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T11:32:19.012435",
     "start_time": "2017-02-17T11:26:21.034200"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model((ind_plain_features.shape[1],))\n",
    "model_eval = evaluate_out_of_domain(model, ind_plain_features, ind_y, ood_plain_features, ood_y)\n",
    "print model_eval\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T14:54:36.345569",
     "start_time": "2017-02-20T14:47:46.506562"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model_sparse((ind_plain_features.shape[1],), 2)\n",
    "model.summary()\n",
    "model_eval = evaluate_out_of_domain(model, \n",
    "                                    [ind_arg_embed, ind_pred_embed, ind_plain_features], ind_y, \n",
    "                                    [ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "print model_eval\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T15:28:29.133470",
     "start_time": "2017-02-20T15:23:21.395468"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model_sparse((ind_plain_features.shape[1],), 1)\n",
    "model.summary()\n",
    "model_eval = evaluate_out_of_domain(model, \n",
    "                                    [ind_arg_embed, ind_plain_features], ind_y, \n",
    "                                    [ood_arg_embed, ood_plain_features], ood_y)\n",
    "print model_eval\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Context model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T15:41:36.049234",
     "start_time": "2017-02-17T15:25:34.248413"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = construct_graph_lstm_model((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "model_eval = evaluate_out_of_domain(model, \n",
    "                                    [ind_arg_embed, ind_pred_embed, ind_plain_features, ind_pred_context], ind_y, \n",
    "                                    [ood_arg_embed, ood_pred_embed, ood_plain_features, ood_pred_context], ood_y)\n",
    "print model_eval\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:15:24.365965",
     "start_time": "2017-03-01T01:15:24.280195"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def select_from_nparray_list(nparray_list, selector):\n",
    "    return [e[selector] for e in nparray_list]\n",
    "\n",
    "\n",
    "train_ids, test_ids = train_test_split(X_orig.ex_id.unique(), test_size=0.2, random_state=42)\n",
    "train_ids = set(train_ids.tolist())\n",
    "test_ids = set(test_ids.tolist())\n",
    "train_selector_pd = X_orig.ex_id.isin(train_ids)\n",
    "test_selector_pd = X_orig.ex_id.isin(test_ids)\n",
    "train_selector = train_selector_pd.as_matrix()\n",
    "test_selector = test_selector_pd.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:24:00.532047",
     "start_time": "2017-03-01T01:24:00.225645"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_data = {'model' : construct_plain_model((plain_features.shape[1],)),\n",
    "              'data' : [plain_features],\n",
    "              'name' : 'simple'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:15:29.346785",
     "start_time": "2017-03-01T01:15:28.797423"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_data = {'model' : construct_plain_model_sparse((plain_features.shape[1],)),\n",
    "              'data' : [embedded_args, embedded_verbs, plain_features],\n",
    "              'name' : 'complex'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-19T22:35:57.781670",
     "start_time": "2017-02-19T22:35:57.303686"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_data = {'model' : construct_plain_model_sparse((no_lemma_plain_features.shape[1],)),\n",
    "              'data' : [embedded_args, embedded_verbs, no_lemma_plain_features],\n",
    "              'name' : 'no_pred_lemma'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:30:00.929123",
     "start_time": "2017-03-01T01:29:06.654882"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = model_data['model']\n",
    "model.summary()\n",
    "model.fit(select_from_nparray_list(model_data['data'], train_selector),\n",
    "          select_from_nparray_list([y], train_selector), \n",
    "          nb_epoch=10, batch_size=64, validation_split = 0.1, shuffle=True, verbose = 0)\n",
    "evaluate_model(model,\n",
    "               select_from_nparray_list(model_data['data'], test_selector), \n",
    "               select_from_nparray_list([y], test_selector)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:25:04.718239",
     "start_time": "2017-03-01T01:25:04.116967"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(select_from_nparray_list(model_data['data'], test_selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:25:16.819504",
     "start_time": "2017-03-01T01:25:04.720227"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_examples_to_store = X_orig.loc[test_selector_pd[test_selector_pd].index, :].loc[:, ['arg_address', 'ex_id']]\n",
    "test_data = {k : data[k] for k in test_ids}\n",
    "\n",
    "for index, (pd_index, row) in enumerate(test_examples_to_store.iterrows()):\n",
    "    ex = test_data[row['ex_id']]\n",
    "    arg_addr = row['arg_address']\n",
    "    sent = ex[arg_addr[0]]\n",
    "    token = sent[arg_addr[1]]\n",
    "    cl = pred[index]\n",
    "    predicted_role = label_encoder.inverse_transform(np.array([cl]))[0]\n",
    "    actual_role = label_encoder.inverse_transform(np.array([select_from_nparray_list([y], test_selector)[0][index]]))[0]\n",
    "\n",
    "    token['rolepred1'] = actual_role\n",
    "    token['rolepred2'] = predicted_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:25:36.285289",
     "start_time": "2017-03-01T01:25:16.822961"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result_path = \"../data/test_data_annotated_with_{}.json\".format(model_data['name'])\n",
    "with open(result_path, 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:25:36.296167",
     "start_time": "2017-03-01T01:25:36.288283"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "def save_model(model, name, model_dir):\n",
    "    output_path = os.path.join(model_dir, name)\n",
    "    with open(output_path + '.json', 'w') as f:\n",
    "        f.write(model.to_json())\n",
    "    \n",
    "    model.save_weights(output_path + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-01T01:25:36.484192",
     "start_time": "2017-03-01T01:25:36.298221"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_path = '../data/models'\n",
    "save_model(model, model_data['name'], model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "51px",
    "width": "313px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "490px",
    "left": "0px",
    "right": "1122px",
    "top": "106px",
    "width": "107px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-11T10:05:24.348564",
     "start_time": "2017-02-11T10:05:24.340293"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('./syntaxnet_wrapper/src')\n",
    "sys.path.append('./pylingtools/src')\n",
    "sys.path.append('./framebank_preprocessing/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-11T10:05:57.834387",
     "start_time": "2017-02-11T10:05:25.083358"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#corpus_path = 'annotated_corpus.json'\n",
    "corpus_path = 'annotated_corpus_fixed.json'\n",
    "\n",
    "with open(corpus_path, 'r') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Removing empty examples\n",
    "new_corp = dict()\n",
    "for exid, example in corpus.iteritems():\n",
    "    new_corp[exid] = [sent for sent in example if sent]\n",
    "\n",
    "fixed_corpus_path = 'annotated_corpus_fixed.json'\n",
    "with open(fixed_corpus_path, 'w') as f:\n",
    "    json.dump(new_corp, f)\n",
    "\n",
    "corpus = new_corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polyglot\n",
      "  Downloading polyglot-16.7.4.tar.gz (126kB)\n",
      "\u001b[K    100% |################################| 133kB 1.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting futures>=2.1.6 (from polyglot)\n",
      "  Downloading futures-3.0.5-py2-none-any.whl\n",
      "Collecting PyICU>=1.8 (from polyglot)\n",
      "  Downloading PyICU-1.9.5.tar.gz (181kB)\n",
      "\u001b[K    100% |################################| 184kB 1.9MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pycld2>=0.3 (from polyglot)\n",
      "  Downloading pycld2-0.31.tar.gz (14.3MB)\n",
      "\u001b[K    100% |################################| 14.3MB 69kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting morfessor>=2.0.2a1 (from polyglot)\n",
      "  Downloading Morfessor-2.0.2a4.tar.gz\n",
      "Requirement already satisfied: six>=1.7.3 in /usr/local/lib/python2.7/dist-packages (from polyglot)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from polyglot)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /usr/lib/python2.7/dist-packages (from polyglot)\n",
      "Building wheels for collected packages: polyglot, PyICU, pycld2, morfessor\n",
      "  Running setup.py bdist_wheel for polyglot ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b4/b7/34/e6fbb82ec71c0c9d7f1b26a038f00129acd99a6aa5e5b93f2d\n",
      "  Running setup.py bdist_wheel for PyICU ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/74/02/1f/ffcfc37347cbada5fdec89bd6c5d6559e825455605cc3a499b\n",
      "  Running setup.py bdist_wheel for pycld2 ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f0/ed/93/4e70bc7622711cb867d8df5bb90ad3bc1bd5cc1659f2dc6c41\n",
      "  Running setup.py bdist_wheel for morfessor ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/95/62/36/a82cf1d3eca449da1272c647b732281be451c91b6b062cc0b0\n",
      "Successfully built polyglot PyICU pycld2 morfessor\n",
      "Installing collected packages: futures, PyICU, pycld2, morfessor, polyglot\n",
      "Successfully installed PyICU-1.9.5 futures-3.0.5 morfessor-2.0.2a4 polyglot-16.7.4 pycld2-0.31\n"
     ]
    }
   ],
   "source": [
    "!pip install polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-11T12:58:54.235976",
     "start_time": "2017-02-11T12:58:54.010281"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue, Manager, current_process\n",
    "from syntaxnet_wrapper import PipelineSyntaxNet\n",
    "from syntaxnet_wrapper import ProcessorSyntaxNet\n",
    "from convert_corpus_to_brat import make_text, create_verb_example_index\n",
    "import math\n",
    "\n",
    "\n",
    "def parse_corpus_syntaxnet2(corpus, proc, addr):\n",
    "    for num, (exid, example) in enumerate(corpus.iteritems()):\n",
    "        raw_input_s = u''\n",
    "        for sent in example:\n",
    "            new_sent = list()\n",
    "            for or_word in sent:\n",
    "                if not or_word['form']:\n",
    "                    new_sent.append(u'_')\n",
    "                elif u' ' in or_word['form'] or u'\\t' in or_word['form']:\n",
    "                    new_sent.append('_')\n",
    "                else:\n",
    "                    new_sent.append(or_word['form'])\n",
    "                    \n",
    "            line = u' '.join(new_sent[:500])\n",
    "            \n",
    "            raw_input_s += line\n",
    "            raw_input_s += u'\\n'\n",
    "            \n",
    "        raw_input_s += u'\\n'\n",
    "    \n",
    "        try:\n",
    "            if num % 10 == 0:\n",
    "                print current_process().name, '--Port: {}--Start:'.format(str(addr)), num, exid \n",
    "            parse_result = proc.parse(raw_input_s)\n",
    "            if num % 10 == 0:\n",
    "                print '--Port: {}--Finished:'.format(str(addr)), num, exid\n",
    "                \n",
    "        except IndexError as err:\n",
    "            print err\n",
    "            print '--Err index error: {}--{}--{}--:'.format(str(addr), num, exid)\n",
    "            \n",
    "        if parse_result is None:\n",
    "            print '--Err: {}--{}--{}--:'.format(str(addr), num, exid)\n",
    "            continue\n",
    "\n",
    "        for sent_num, parse_sent in enumerate(parse_result):\n",
    "            ex_sent = example[sent_num]\n",
    "            \n",
    "            if len(parse_sent) != len(ex_sent) and len(ex_sent) <= 500:\n",
    "                print sent_num\n",
    "                print len(parse_sent)\n",
    "                print len(ex_sent)\n",
    "                print exid\n",
    "            \n",
    "            for word_num, parse_word in enumerate(parse_sent):\n",
    "                corp_word = example[sent_num][word_num]\n",
    "                corp_word['feat'] = parse_word.morph\n",
    "                corp_word['postag'] = parse_word.pos_tag\n",
    "                corp_word['parent'] = parse_word.parent \n",
    "                corp_word['link_name'] = parse_word.link_name\n",
    "\n",
    "\n",
    "def process_chunk(chunk, ppl, addr, out_q):\n",
    "    parse_corpus_syntaxnet2(chunk, ppl, addr)\n",
    "    out_q.put(chunk)\n",
    "    print 'Process finished:', current_process().name\n",
    "    \n",
    "\n",
    "class MultiprocessParser(object):\n",
    "    def __init__(self, addresses = []):\n",
    "        super(MultiprocessParser, self).__init__()\n",
    "        self.addresses_ = addresses\n",
    "        self.out_q = Manager().Queue()\n",
    "    \n",
    "    def _make_chunks(self, corpus):\n",
    "        ex_keys = corpus.keys()\n",
    "        if len(self.addresses_) == 1:\n",
    "            chunk_size = len(corpus)\n",
    "        else:\n",
    "            chunk_size = int(math.ceil((1. * len(corpus) / len(self.addresses_))))\n",
    "            new_size = chunk_size * len(self.addresses_) - len(corpus)\n",
    "            chunks = [ex_keys[x : x + chunk_size - 1] for x in xrange(0, new_size * (chunk_size - 1), chunk_size - 1)]\n",
    "            chunks += [ex_keys[x : x + chunk_size] for x in xrange((chunk_size - 1) * new_size, len(corpus), chunk_size)]\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def process(self, corpus):\n",
    "        chunks = self._make_chunks(corpus)\n",
    "        \n",
    "        proc_pool = list()\n",
    "        for chunk, addr in zip(chunks, self.addresses_):\n",
    "            dict_chunk = {k : corpus[k] for k in chunk}\n",
    "            ppl = ProcessorSyntaxNet(addr[0], addr[1])\n",
    "            proc = Process(target=process_chunk, args=(dict_chunk, \n",
    "                                                       ppl, addr, self.out_q))\n",
    "            proc.start()\n",
    "            proc_pool.append(proc)\n",
    "        \n",
    "        for proc in proc_pool:\n",
    "            proc.join()\n",
    "            print proc\n",
    "        \n",
    "\n",
    "        tmp = list()\n",
    "        while not self.out_q.empty():\n",
    "            tmp.append(self.out_q.get())\n",
    "        \n",
    "        result = {}\n",
    "        for chunk in tmp:\n",
    "            result.update(chunk)\n",
    "            \n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-02-11T00:12:09.475Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Place hosts with containers here\n",
    "# Found bug with more than 15-20 processes, use <= 20\n",
    "hosts = [('myhost', port) for port in xrange(8200, 8215)]\n",
    "mproc = MultiprocessParser([('vmh1.isa.ru', port) for port in xrange(8200, 8215)])\n",
    "result = mproc.process(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(len(corpus) == len(result)) # Be aware of connection reset by peer problem if >20 processess are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_file_path = 'annotated_corpus_fixed+syntaxnet.json'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
